{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pn7PKu9r0asK"
      },
      "source": [
        "# Tutorial 8 - Options\n",
        "\n",
        "Please complete this tutorial to get an overview of options and an implementation of SMDP Q-Learning and Intra-Option Q-Learning.\n",
        "\n",
        "\n",
        "### References:\n",
        "\n",
        " [Recent Advances in Hierarchical Reinforcement\n",
        "Learning](https://people.cs.umass.edu/~mahadeva/papers/hrl.pdf) is a strong recommendation for topics in HRL that was covered in class. Watch Prof. Ravi's lectures on moodle or nptel for further understanding the core concepts. Contact the TAs for further resources if needed.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy==1.23"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M7VFHIUuBIAz",
        "outputId": "a2bc2834-59b8-46ba-ef1c-9b79730e0913"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy==1.23 in /usr/local/lib/python3.10/dist-packages (1.23.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gym==0.22"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XMlL1eI2RBnM",
        "outputId": "ff4f548e-42df-4f32-f104-544043e4acac"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gym==0.22\n",
            "  Downloading gym-0.22.0.tar.gz (631 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m631.1/631.1 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym==0.22) (1.23.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym==0.22) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym==0.22) (0.0.8)\n",
            "Building wheels for collected packages: gym\n",
            "  Building wheel for gym (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.22.0-py3-none-any.whl size=708362 sha256=6639a8a65776e73e52aa3b1e41b5d6fd26e9447de6f482e35aff13fbce3e440d\n",
            "  Stored in directory: /root/.cache/pip/wheels/42/e8/e8/6dfbc92a1dcd76c1a5e2bb982750fd6b7e792239f46039e6b1\n",
            "Successfully built gym\n",
            "Installing collected packages: gym\n",
            "  Attempting uninstall: gym\n",
            "    Found existing installation: gym 0.25.2\n",
            "    Uninstalling gym-0.25.2:\n",
            "      Successfully uninstalled gym-0.25.2\n",
            "Successfully installed gym-0.22.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "P_DODRgW_ZKS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e83999b4-f148-4362-9a6a-2d05da7676a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "A bunch of imports, you don't have to worry about these\n",
        "'''\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "import gym\n",
        "from gym.wrappers import Monitor\n",
        "import glob\n",
        "import io\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import HTML"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fYNA5kiH_esJ",
        "outputId": "f0ba20f8-1e36-4cc5-c413-a60a4c41056c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "36\n",
            "Number of states: 48\n",
            "Number of actions that an agent can take: 4\n",
            "Action taken: left\n",
            "Transition probability: {'prob': 1.0}\n",
            "Next state: 36\n",
            "Reward recieved: -1\n",
            "Terminal state: False\n",
            "o  o  o  o  o  o  o  o  o  o  o  o\n",
            "o  o  o  o  o  o  o  o  o  o  o  o\n",
            "o  o  o  o  o  o  o  o  o  o  o  o\n",
            "x  C  C  C  C  C  C  C  C  C  C  T\n",
            "\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "The environment used here is extremely similar to the openai gym ones.\n",
        "At first glance it might look slightly different.\n",
        "The usual commands we use for our experiments are added to this cell to aid you\n",
        "work using this environment.\n",
        "'''\n",
        "\n",
        "#Setting up the environment\n",
        "from gym.envs.toy_text.cliffwalking import CliffWalkingEnv\n",
        "env = CliffWalkingEnv()\n",
        "\n",
        "env.reset()\n",
        "\n",
        "#Current State\n",
        "print(env.s)\n",
        "\n",
        "# 4x12 grid = 48 states\n",
        "print (\"Number of states:\", env.nS)\n",
        "\n",
        "# Primitive Actions\n",
        "action = [\"up\", \"right\", \"down\", \"left\"]\n",
        "#correspond to [0,1,2,3] that's actually passed to the environment\n",
        "\n",
        "# either go left, up, down or right\n",
        "print (\"Number of actions that an agent can take:\", env.nA)\n",
        "\n",
        "# Example Transitions\n",
        "rnd_action = random.randint(0, 3)\n",
        "print (\"Action taken:\", action[rnd_action])\n",
        "next_state, reward, is_terminal, t_prob = env.step(rnd_action)\n",
        "print (\"Transition probability:\", t_prob)\n",
        "print (\"Next state:\", next_state)\n",
        "print (\"Reward recieved:\", reward)\n",
        "print (\"Terminal state:\", is_terminal)\n",
        "env.render()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apuaOxavDXus"
      },
      "source": [
        "#### Options\n",
        "We custom define very simple options here. They might not be the logical options for this settings deliberately chosen to visualise the Q Table better.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "g4MRC1p2DZbp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "c1e76716-698d-458a-e362-b20cd6858238"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nNow the new action space will contain\\nPrimitive Actions: [\"up\", \"right\", \"down\", \"left\"]\\nOptions: [\"Away\",\"Close\"]\\nTotal Actions :[\"up\", \"right\", \"down\", \"left\", \"Away\", \"Close\"]\\nCorresponding to [0,1,2,3,4,5]\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# We are defining two more options here\n",
        "# Option 1 [\"Away\"] - > Away from Cliff (ie keep going up)\n",
        "# Option 2 [\"Close\"] - > Close to Cliff (ie keep going down)\n",
        "\n",
        "def Away(env,state):\n",
        "\n",
        "    optdone = False\n",
        "    optact = 0\n",
        "\n",
        "    if (int(state/12) == 0):\n",
        "        optdone = True\n",
        "\n",
        "    return [optact,optdone]\n",
        "\n",
        "def Close(env,state):\n",
        "\n",
        "    optdone = False\n",
        "    optact = 2\n",
        "\n",
        "    if (int(state/12) == 2):\n",
        "        optdone = True\n",
        "\n",
        "    return [optact,optdone]\n",
        "\n",
        "\n",
        "'''\n",
        "Now the new action space will contain\n",
        "Primitive Actions: [\"up\", \"right\", \"down\", \"left\"]\n",
        "Options: [\"Away\",\"Close\"]\n",
        "Total Actions :[\"up\", \"right\", \"down\", \"left\", \"Away\", \"Close\"]\n",
        "Corresponding to [0,1,2,3,4,5]\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jmv5c0XoK8GA"
      },
      "source": [
        "# Task 1\n",
        "Complete the code cell below\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "bh_oghc7Ledh"
      },
      "outputs": [],
      "source": [
        "# epsilon-greedy action selection function\n",
        "def egreedy_policy(q_values, state, epsilon):\n",
        "    if np.random.rand() < epsilon:\n",
        "        return np.random.randint(6)\n",
        "    else:\n",
        "        return np.argmax(q_values[state])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8VJYkqoLqlO"
      },
      "source": [
        "# Task 2\n",
        "Below is an incomplete code cell with the flow of SMDP Q-Learning. Complete the cell and train the agent using SMDP Q-Learning algorithm.\n",
        "Keep the **final Q-table** and **Update Frequency** table handy (You'll need it in TODO 4)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### SMDP Q-Learning\n",
        "update_frequency_smdp = np.zeros((env.nS, 6))  # For tracking update frequency\n",
        "cumulative_rewards_smdp = []  # For tracking cumulative rewards\n",
        "# Parameters\n",
        "gamma = 0.9\n",
        "alpha = 0.1\n",
        "epsilon = 0.1\n",
        "\n",
        "# Q-Values initialization\n",
        "q_values_SMDP = np.zeros((48, 6))\n",
        "\n",
        "# SMDP Q-Learning with Update Frequency\n",
        "for episode in range(1000):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "\n",
        "    for _ in range(1000):\n",
        "        action = egreedy_policy(q_values_SMDP, state, epsilon)\n",
        "\n",
        "        if action < 4:  # Primitive action\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            best_next_action = np.argmax(q_values_SMDP[next_state])\n",
        "            q_values_SMDP[state, action] += alpha * (reward + gamma * q_values_SMDP[next_state, best_next_action] - q_values_SMDP[state, action])\n",
        "            update_frequency_smdp[state, action] += 1  # Update frequency\n",
        "            state = next_state\n",
        "        else:\n",
        "            reward_bar = 0\n",
        "            beta = 1  # Discounting over steps within the option\n",
        "            optdone = False\n",
        "            # while not optdone and not done:\n",
        "            for _ in range(1000):\n",
        "                if action == 4:  # \"Away\" option\n",
        "                    optact, optdone = Away(env, state)\n",
        "                elif action == 5:  # \"Close\" option\n",
        "                    optact, optdone = Close(env, state)\n",
        "\n",
        "                next_state, reward, done, _ = env.step(optact)\n",
        "                reward_bar += beta * reward\n",
        "                beta *= gamma\n",
        "                state = next_state\n",
        "                if done or optdone:\n",
        "                  break\n",
        "\n",
        "            q_values_SMDP[state, action] += alpha * (reward_bar - q_values_SMDP[state, action])\n",
        "            update_frequency_smdp[state, action] += 1  # Update frequency\n",
        "\n",
        "        total_reward += reward\n",
        "        if done:\n",
        "          break\n",
        "\n",
        "    cumulative_rewards_smdp.append(total_reward)\n"
      ],
      "metadata": {
        "id": "rowdPoWVy2-D"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3SQFbRCHWQyO"
      },
      "source": [
        "# Task 3\n",
        "Using the same options and the SMDP code, implement Intra Option Q-Learning (In the code cell below). You *might not* always have to search through options to find the options with similar policies, think about it. Keep the **final Q-table** and **Update Frequency** table handy (You'll need it in TODO 4)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### Intra-Option Q-Learning\n",
        "q_values_IOQL = np.zeros((48, 6))\n",
        "update_frequency_ioql = np.zeros((env.nS, 6))  # For tracking update frequency\n",
        "cumulative_rewards_intra_option = []  # For tracking cumulative rewards\n",
        "\n",
        "# Intra-Option Q-Learning with Update Frequency\n",
        "for episode in range(1000):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "\n",
        "    for i in range(1000):\n",
        "        action = egreedy_policy(q_values_IOQL, state, epsilon)\n",
        "\n",
        "        if action < 4:  # Primitive action\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            best_next_action = np.argmax(q_values_IOQL[next_state])\n",
        "            q_values_IOQL[state, action] += alpha * (reward + gamma * q_values_IOQL[next_state, best_next_action] - q_values_IOQL[state, action])\n",
        "            update_frequency_ioql[state, action] += 1  # Update frequency\n",
        "            state = next_state\n",
        "        else:\n",
        "            optdone = False\n",
        "            # while not optdone and not done:\n",
        "            for _ in range(1000):\n",
        "                if action == 4:  # \"Away\" option\n",
        "                    optact, optdone = Away(env, state)\n",
        "                elif action == 5:  # \"Close\" option\n",
        "                    optact, optdone = Close(env, state)\n",
        "\n",
        "                next_state, reward, done, _ = env.step(optact)\n",
        "                best_next_action = np.argmax(q_values_IOQL[next_state])\n",
        "                q_values_IOQL[state, action] += alpha * (reward + gamma * q_values_IOQL[next_state, best_next_action] - q_values_IOQL[state, action])\n",
        "                update_frequency_ioql[state, action] += 1  # Update frequency\n",
        "                state = next_state\n",
        "                if done or optdone:\n",
        "                  break\n",
        "\n",
        "        total_reward += reward\n",
        "        if done:\n",
        "          break\n",
        "\n",
        "    cumulative_rewards_intra_option.append(total_reward)\n"
      ],
      "metadata": {
        "id": "T_syDiyazBu9"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzUgcwL-VfkO"
      },
      "source": [
        "# Task 4\n",
        "Compare the two Q-Tables and Update Frequencies and provide comments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "v8mZE74_Vhmg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67afc08b-5b09-453e-d972-498712814aa5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparison of final Q-tables:\n",
            "SMDP Q-Learning Q-table:\n",
            "[[  -1.           -1.           -1.           -1.           -3.42462733\n",
            "     0.        ]\n",
            " [  -0.99999983   -0.99999999   -0.99999994   -0.99999952   -1.98603141\n",
            "     0.        ]\n",
            " [  -0.271        -0.19         -0.3439       -0.5217031    -1.19542509\n",
            "     0.        ]\n",
            " [  -0.1          -0.1          -0.1          -0.1          -0.1\n",
            "     0.        ]\n",
            " [  -0.1          -0.1          -0.1          -0.1           0.\n",
            "     0.        ]\n",
            " [  -0.1          -0.1          -0.1          -0.1           0.\n",
            "     0.        ]\n",
            " [  -0.1          -0.1          -0.1          -0.1           0.\n",
            "     0.        ]\n",
            " [  -0.1          -0.1          -0.1          -0.1           0.\n",
            "     0.        ]\n",
            " [  -0.1          -0.1          -0.1          -0.1           0.\n",
            "     0.        ]\n",
            " [  -0.1          -0.1          -0.1          -0.1           0.\n",
            "     0.        ]\n",
            " [  -0.1          -0.1          -0.1          -0.1           0.\n",
            "     0.        ]\n",
            " [  -0.1          -0.1          -0.1          -0.1           0.\n",
            "     0.        ]\n",
            " [  -0.99999946   -0.99999738   -0.99999961   -0.99999985    0.\n",
            "     0.        ]\n",
            " [  -0.40951      -0.5217031    -0.61257951   -0.1           0.\n",
            "     0.        ]\n",
            " [  -0.1          -0.1          -0.19         -0.1           0.\n",
            "     0.        ]\n",
            " [  -0.1          -0.1          -0.1          -0.1           0.\n",
            "     0.        ]\n",
            " [  -0.1          -0.1          -0.1          -0.1           0.\n",
            "     0.        ]\n",
            " [  -0.1          -0.1          -0.1          -0.1           0.\n",
            "     0.        ]\n",
            " [  -0.1          -0.1          -0.1          -0.1           0.\n",
            "     0.        ]\n",
            " [  -0.1          -0.1          -0.1          -0.1           0.\n",
            "     0.        ]\n",
            " [  -0.1          -0.1          -0.1          -0.1           0.\n",
            "     0.        ]\n",
            " [  -0.1          -0.1          -0.1          -0.1           0.\n",
            "     0.        ]\n",
            " [  -0.1          -0.1          -0.1          -0.1           0.\n",
            "     0.        ]\n",
            " [  -0.1          -0.1          -0.1          -0.1           0.\n",
            "     0.        ]\n",
            " [  -0.99999709   -0.99999986   -0.99999993   -0.9999994     0.\n",
            "     0.        ]\n",
            " [  -0.271        -0.271       -40.951        -0.19          0.\n",
            "     0.        ]\n",
            " [  -0.19         -0.1         -10.            0.            0.\n",
            "     0.        ]\n",
            " [  -0.1          -0.1           0.            0.            0.\n",
            "     0.        ]\n",
            " [  -0.1          -0.1           0.            0.            0.\n",
            "     0.        ]\n",
            " [  -0.1          -0.1           0.            0.            0.\n",
            "     0.        ]\n",
            " [  -0.1          -0.1           0.            0.            0.\n",
            "     0.        ]\n",
            " [  -0.1          -0.1           0.            0.            0.\n",
            "     0.        ]\n",
            " [  -0.1          -0.1           0.            0.            0.\n",
            "     0.        ]\n",
            " [  -0.1          -0.1           0.            0.            0.\n",
            "     0.        ]\n",
            " [  -0.1          -0.1           0.            0.            0.\n",
            "     0.        ]\n",
            " [  -0.1          -0.1          -0.1           0.            0.\n",
            "     0.        ]\n",
            " [  -1.         -100.           -1.           -1.            0.\n",
            "    -3.29339625]\n",
            " [   0.            0.            0.            0.            0.\n",
            "     0.        ]\n",
            " [   0.            0.            0.            0.            0.\n",
            "     0.        ]\n",
            " [   0.            0.            0.            0.            0.\n",
            "     0.        ]\n",
            " [   0.            0.            0.            0.            0.\n",
            "     0.        ]\n",
            " [   0.            0.            0.            0.            0.\n",
            "     0.        ]\n",
            " [   0.            0.            0.            0.            0.\n",
            "     0.        ]\n",
            " [   0.            0.            0.            0.            0.\n",
            "     0.        ]\n",
            " [   0.            0.            0.            0.            0.\n",
            "     0.        ]\n",
            " [   0.            0.            0.            0.            0.\n",
            "     0.        ]\n",
            " [   0.            0.            0.            0.            0.\n",
            "     0.        ]\n",
            " [   0.            0.            0.            0.            0.\n",
            "     0.        ]]\n",
            "\n",
            "Intra-Option Q-Learning Q-table:\n",
            "[[  -7.66813894   -7.63634765   -7.63625949   -7.71313663   -7.869422\n",
            "    -7.63833348]\n",
            " [  -7.41242549   -7.37927809   -7.37922929   -7.53451083   -7.53713038\n",
            "    -7.37945131]\n",
            " [  -7.14543252   -7.09508646   -7.09832242   -7.15982086   -7.30063433\n",
            "    -7.09610397]\n",
            " [  -6.81151791   -6.77965826   -6.78562903   -7.01178753   -6.95936416\n",
            "    -6.78255027]\n",
            " [  -6.51634863   -6.43005813   -6.44217148   -6.58814186   -6.53726007\n",
            "    -6.43330216]\n",
            " [  -6.05083316   -6.04690732   -6.05034874   -6.25751876   -6.34267214\n",
            "    -6.04873472]\n",
            " [  -5.71605594   -5.62142424   -5.62043878   -5.71438801   -5.97383286\n",
            "    -5.62504892]\n",
            " [  -5.17219261   -5.1483253    -5.1485301    -5.15868614   -5.31928706\n",
            "    -5.1528261 ]\n",
            " [  -4.67164374   -4.62691402   -4.62890536   -4.68969221   -4.97990226\n",
            "    -4.62652174]\n",
            " [  -4.05258316   -4.04658699   -4.05041125   -4.32209072   -4.47944405\n",
            "    -4.04883142]\n",
            " [  -3.42930797   -3.40232993   -3.40351087   -3.47865585   -3.70537054\n",
            "    -3.40549993]\n",
            " [  -2.74206096   -2.74724097   -2.69344517   -2.80434205   -3.13190614\n",
            "    -2.69430473]\n",
            " [  -7.45403854   -7.40812062   -7.41026511   -7.44186478   -7.86941428\n",
            "    -7.45810456]\n",
            " [  -7.22897491   -7.14371613   -7.14410723   -7.15209677   -7.49119702\n",
            "    -7.17453148]\n",
            " [  -6.86026055   -6.8377241    -6.83990758   -6.93867095   -7.26684627\n",
            "    -6.85794495]\n",
            " [  -6.63311487   -6.49477012   -6.49571216   -6.75768113   -6.85954314\n",
            "    -6.50818899]\n",
            " [  -6.15960045   -6.11247306   -6.11327803   -6.14433361   -6.4031369\n",
            "    -6.11721195]\n",
            " [  -5.78325174   -5.68568769   -5.68570948   -5.83113348   -6.24751991\n",
            "    -5.68810706]\n",
            " [  -5.37860746   -5.20970643   -5.20944492   -5.40822225   -5.9267941\n",
            "    -5.21007215]\n",
            " [  -4.88277009   -4.68030096   -4.68023199   -4.87804781   -5.16439596\n",
            "    -4.6804695 ]\n",
            " [  -4.46598678   -4.09149513   -4.09167871   -4.25931402   -4.85374364\n",
            "    -4.09187752]\n",
            " [  -3.51985291   -3.43701087   -3.43714254   -3.83105002   -4.37315371\n",
            "    -3.43722044]\n",
            " [  -2.74082055   -2.7092188    -2.70921781   -2.84579691   -3.44729981\n",
            "    -2.70935282]\n",
            " [  -2.05364586   -1.98342513   -1.89996389   -2.00589225   -2.96484354\n",
            "    -1.89996401]\n",
            " [  -7.57374759   -7.17570464   -7.61726774   -7.41202251   -7.66142626\n",
            "    -7.71231526]\n",
            " [  -7.28762082   -6.86189404 -100.27068387   -7.27309483   -7.27256092\n",
            "  -106.71208967]\n",
            " [  -7.00560694   -6.5132156   -98.91721169   -7.08271437   -7.02644226\n",
            "  -106.71206116]\n",
            " [  -6.6664285    -6.12579511 -101.49643683   -6.76441556   -6.64555367\n",
            "  -106.71125183]\n",
            " [  -6.23055555   -5.6953279  -101.97295985   -6.41142653   -6.16655613\n",
            "  -106.71062512]\n",
            " [  -5.96235878   -5.217031    -79.29823843   -6.00678725   -5.91753485\n",
            "  -106.70783254]\n",
            " [  -5.54250895   -4.68559     -96.08627813   -5.47242486   -5.58298844\n",
            "  -106.70881785]\n",
            " [  -5.05468215   -4.0951      -95.95800879   -4.97367844   -4.82563869\n",
            "  -106.70962356]\n",
            " [  -4.38531276   -3.439       -98.14505633   -4.34800172   -4.40236926\n",
            "  -106.70926872]\n",
            " [  -3.81209791   -2.71        -93.63845325   -3.96715699   -3.86884252\n",
            "  -106.71106593]\n",
            " [  -3.17487719   -1.9         -82.22014735   -3.04180755   -2.85442322\n",
            "  -106.71091825]\n",
            " [  -2.5360875    -1.73996286   -1.           -2.35659088   -2.29632296\n",
            "    -1.        ]\n",
            " [  -7.45813417 -106.59717053   -7.69579861   -7.70563347   -7.45813417\n",
            "    -7.71232075]\n",
            " [   0.            0.            0.            0.            0.\n",
            "     0.        ]\n",
            " [   0.            0.            0.            0.            0.\n",
            "     0.        ]\n",
            " [   0.            0.            0.            0.            0.\n",
            "     0.        ]\n",
            " [   0.            0.            0.            0.            0.\n",
            "     0.        ]\n",
            " [   0.            0.            0.            0.            0.\n",
            "     0.        ]\n",
            " [   0.            0.            0.            0.            0.\n",
            "     0.        ]\n",
            " [   0.            0.            0.            0.            0.\n",
            "     0.        ]\n",
            " [   0.            0.            0.            0.            0.\n",
            "     0.        ]\n",
            " [   0.            0.            0.            0.            0.\n",
            "     0.        ]\n",
            " [   0.            0.            0.            0.            0.\n",
            "     0.        ]\n",
            " [   0.            0.            0.            0.            0.\n",
            "     0.        ]]\n",
            "\n",
            "Comparison of update frequencies:\n",
            "SMDP Q-Learning Update Frequency:\n",
            "[[8.21200e+03 8.21700e+03 8.00500e+03 8.15700e+03 4.69911e+05 0.00000e+00]\n",
            " [1.48000e+02 1.73000e+02 1.58000e+02 1.38000e+02 5.40000e+02 0.00000e+00]\n",
            " [3.00000e+00 2.00000e+00 4.00000e+00 7.00000e+00 1.20000e+01 0.00000e+00]\n",
            " [1.00000e+00 1.00000e+00 1.00000e+00 1.00000e+00 1.00000e+00 0.00000e+00]\n",
            " [1.00000e+00 1.00000e+00 1.00000e+00 1.00000e+00 0.00000e+00 0.00000e+00]\n",
            " [1.00000e+00 1.00000e+00 1.00000e+00 1.00000e+00 0.00000e+00 0.00000e+00]\n",
            " [1.00000e+00 1.00000e+00 1.00000e+00 1.00000e+00 0.00000e+00 0.00000e+00]\n",
            " [1.00000e+00 1.00000e+00 1.00000e+00 1.00000e+00 0.00000e+00 0.00000e+00]\n",
            " [1.00000e+00 1.00000e+00 1.00000e+00 1.00000e+00 0.00000e+00 0.00000e+00]\n",
            " [1.00000e+00 1.00000e+00 1.00000e+00 1.00000e+00 0.00000e+00 0.00000e+00]\n",
            " [1.00000e+00 1.00000e+00 1.00000e+00 1.00000e+00 0.00000e+00 0.00000e+00]\n",
            " [1.00000e+00 1.00000e+00 1.00000e+00 1.00000e+00 0.00000e+00 0.00000e+00]\n",
            " [1.37000e+02 1.22000e+02 1.40000e+02 1.49000e+02 0.00000e+00 0.00000e+00]\n",
            " [5.00000e+00 7.00000e+00 9.00000e+00 1.00000e+00 0.00000e+00 0.00000e+00]\n",
            " [1.00000e+00 1.00000e+00 2.00000e+00 1.00000e+00 0.00000e+00 0.00000e+00]\n",
            " [1.00000e+00 1.00000e+00 1.00000e+00 1.00000e+00 0.00000e+00 0.00000e+00]\n",
            " [1.00000e+00 1.00000e+00 1.00000e+00 1.00000e+00 0.00000e+00 0.00000e+00]\n",
            " [1.00000e+00 1.00000e+00 1.00000e+00 1.00000e+00 0.00000e+00 0.00000e+00]\n",
            " [1.00000e+00 1.00000e+00 1.00000e+00 1.00000e+00 0.00000e+00 0.00000e+00]\n",
            " [1.00000e+00 1.00000e+00 1.00000e+00 1.00000e+00 0.00000e+00 0.00000e+00]\n",
            " [1.00000e+00 1.00000e+00 1.00000e+00 1.00000e+00 0.00000e+00 0.00000e+00]\n",
            " [1.00000e+00 1.00000e+00 1.00000e+00 1.00000e+00 0.00000e+00 0.00000e+00]\n",
            " [1.00000e+00 1.00000e+00 1.00000e+00 1.00000e+00 0.00000e+00 0.00000e+00]\n",
            " [1.00000e+00 1.00000e+00 1.00000e+00 1.00000e+00 0.00000e+00 0.00000e+00]\n",
            " [1.21000e+02 1.50000e+02 1.56000e+02 1.36000e+02 0.00000e+00 0.00000e+00]\n",
            " [3.00000e+00 3.00000e+00 5.00000e+00 2.00000e+00 0.00000e+00 0.00000e+00]\n",
            " [2.00000e+00 1.00000e+00 1.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00]\n",
            " [1.00000e+00 1.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00]\n",
            " [1.00000e+00 1.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00]\n",
            " [1.00000e+00 1.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00]\n",
            " [1.00000e+00 1.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00]\n",
            " [1.00000e+00 1.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00]\n",
            " [1.00000e+00 1.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00]\n",
            " [1.00000e+00 1.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00]\n",
            " [1.00000e+00 1.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00]\n",
            " [1.00000e+00 1.00000e+00 1.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00]\n",
            " [8.13800e+03 8.08200e+03 8.08600e+03 8.07400e+03 0.00000e+00 4.62054e+05]\n",
            " [0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00]\n",
            " [0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00]\n",
            " [0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00]\n",
            " [0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00]\n",
            " [0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00]\n",
            " [0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00]\n",
            " [0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00]\n",
            " [0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00]\n",
            " [0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00]\n",
            " [0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00]\n",
            " [0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00]]\n",
            "\n",
            "Intra-Option Q-Learning Update Frequency:\n",
            "[[1.470e+02 1.050e+03 3.180e+02 1.490e+02 1.528e+03 3.160e+02]\n",
            " [1.360e+02 8.760e+02 2.110e+02 8.600e+01 1.760e+02 2.100e+02]\n",
            " [1.260e+02 7.620e+02 1.740e+02 7.600e+01 1.610e+02 1.740e+02]\n",
            " [1.150e+02 6.520e+02 1.520e+02 7.500e+01 1.370e+02 1.510e+02]\n",
            " [1.060e+02 5.570e+02 1.370e+02 6.600e+01 1.220e+02 1.350e+02]\n",
            " [9.300e+01 4.720e+02 1.220e+02 6.100e+01 1.140e+02 1.210e+02]\n",
            " [8.500e+01 4.020e+02 1.090e+02 5.300e+01 1.090e+02 1.090e+02]\n",
            " [7.300e+01 3.240e+02 9.800e+01 4.600e+01 8.100e+01 9.800e+01]\n",
            " [6.300e+01 2.580e+02 8.800e+01 4.100e+01 7.800e+01 8.700e+01]\n",
            " [5.200e+01 1.840e+02 7.900e+01 3.700e+01 7.000e+01 7.800e+01]\n",
            " [4.200e+01 1.050e+02 7.000e+01 2.900e+01 5.100e+01 7.000e+01]\n",
            " [3.200e+01 3.200e+01 6.500e+01 2.500e+01 4.200e+01 6.500e+01]\n",
            " [9.600e+01 3.640e+02 1.170e+02 1.360e+02 1.502e+03 3.290e+02]\n",
            " [1.000e+02 4.380e+02 1.320e+02 8.200e+01 1.580e+02 2.250e+02]\n",
            " [9.500e+01 4.550e+02 1.390e+02 7.400e+01 1.400e+02 1.870e+02]\n",
            " [9.200e+01 4.410e+02 1.390e+02 7.000e+01 1.100e+02 1.620e+02]\n",
            " [8.200e+01 4.070e+02 1.350e+02 5.900e+01 1.000e+02 1.510e+02]\n",
            " [7.400e+01 3.700e+02 1.280e+02 5.500e+01 9.300e+01 1.380e+02]\n",
            " [6.700e+01 3.290e+02 1.200e+02 4.900e+01 8.800e+01 1.270e+02]\n",
            " [5.800e+01 2.840e+02 1.120e+02 4.200e+01 6.600e+01 1.170e+02]\n",
            " [5.100e+01 2.300e+02 1.040e+02 3.600e+01 6.200e+01 1.080e+02]\n",
            " [3.700e+01 1.730e+02 9.800e+01 3.200e+01 5.400e+01 1.020e+02]\n",
            " [2.700e+01 1.110e+02 9.400e+01 2.300e+01 3.800e+01 9.900e+01]\n",
            " [1.900e+01 2.200e+01 1.130e+02 1.700e+01 3.100e+01 1.140e+02]\n",
            " [1.980e+02 1.702e+03 9.000e+01 1.440e+02 1.494e+03 3.670e+02]\n",
            " [1.380e+02 1.505e+03 2.700e+01 7.900e+01 1.380e+02 2.530e+02]\n",
            " [1.160e+02 1.337e+03 2.500e+01 8.100e+01 1.180e+02 2.300e+02]\n",
            " [9.900e+01 1.211e+03 2.900e+01 7.600e+01 9.800e+01 1.960e+02]\n",
            " [8.400e+01 1.116e+03 3.000e+01 7.100e+01 8.200e+01 1.790e+02]\n",
            " [8.100e+01 1.047e+03 1.300e+01 6.500e+01 7.800e+01 1.520e+02]\n",
            " [7.400e+01 9.730e+02 2.200e+01 5.400e+01 7.600e+01 1.430e+02]\n",
            " [6.400e+01 9.200e+02 2.200e+01 4.800e+01 5.400e+01 1.340e+02]\n",
            " [5.000e+01 8.920e+02 2.400e+01 4.000e+01 5.000e+01 1.200e+02]\n",
            " [4.200e+01 8.480e+02 2.000e+01 4.400e+01 4.400e+01 1.220e+02]\n",
            " [3.500e+01 8.460e+02 1.400e+01 2.800e+01 2.700e+01 1.140e+02]\n",
            " [3.100e+01 2.700e+01 6.370e+02 2.300e+01 2.200e+01 3.600e+02]\n",
            " [1.861e+03 6.800e+01 1.650e+02 1.730e+02 1.465e+03 6.800e+04]\n",
            " [0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00]\n",
            " [0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00]\n",
            " [0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00]\n",
            " [0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00]\n",
            " [0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00]\n",
            " [0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00]\n",
            " [0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00]\n",
            " [0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00]\n",
            " [0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00]\n",
            " [0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00]\n",
            " [0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00]]\n",
            "\n",
            "Comments:\n",
            "1. Convergence Speed: Check how quickly each algorithm's Q-table converges towards optimal values. Faster convergence indicates better learning.\n",
            "2. Learned Policies: Analyze the learned policies from both algorithms. Are they similar or different?\n",
            "3. Exploration of State-Action Space: Determine how efficiently each algorithm explores the state-action space. Do they cover all possible actions and states?\n",
            "4. Utilization of Options: Assess how effectively each algorithm utilizes the defined options in the environment. Do options improve learning performance?\n"
          ]
        }
      ],
      "source": [
        "# Use this cell for Task 4 Code# Task 4\n",
        "# Compare the two Q-Tables and Update Frequencies and provide comments.\n",
        "\n",
        "# Compare final Q-tables\n",
        "print(\"Comparison of final Q-tables:\")\n",
        "print(\"SMDP Q-Learning Q-table:\")\n",
        "print(q_values_SMDP)\n",
        "print(\"\\nIntra-Option Q-Learning Q-table:\")\n",
        "print(q_values_IOQL)\n",
        "\n",
        "# Compare update frequencies\n",
        "print(\"\\nComparison of update frequencies:\")\n",
        "print(\"SMDP Q-Learning Update Frequency:\")\n",
        "print(update_frequency_smdp)\n",
        "print(\"\\nIntra-Option Q-Learning Update Frequency:\")\n",
        "print(update_frequency_ioql )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SemE13ORV04n"
      },
      "source": [
        "Use this text cell for your comments - Task 4\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Inference**\n",
        "\n"
      ],
      "metadata": {
        "id": "2Qq0OM7nUOmH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Final Q-Tables Analysis:\n",
        "\n",
        "- **SMDP Q-Learning Q-table** suggests a strong distinction between the values assigned to primitive actions (up, right, down, left) and the two options (\"Away\" and \"Close\"). For many states, the options have significantly different Q-values compared to primitive actions, indicating that the algorithm has learned when it's advantageous to execute these options.\n",
        "\n",
        "- **Intra-Option Q-Learning Q-table** shows a more varied distribution of Q-values across both primitive actions and options. This indicates a more dynamic use of both options and primitive actions across different states.\n",
        "\n",
        "### Update Frequencies Analysis:\n",
        "\n",
        "- **SMDP Q-Learning Update Frequency** shows a high frequency of updates for the options compared to primitive actions in certain states, which indicates that the options were utilized extensively during the learning process.\n",
        "\n",
        "- **Intra-Option Q-Learning Update Frequency** displays a more balanced update frequency between primitive actions and options, which indicates more exploratory behaviore not just with options but also with primitive actions.\n",
        "\n",
        "### Inference:\n",
        "- **SMDP Q-Learning** seems to prioritize options, potentially at the expense of exploring primitive actions, which might limit policy flexibility.\n",
        "- **Intra-Option Q-Learning** provides a more balanced and nuanced approach, leveraging both options and primitive actions for a potentially more adaptable and refined policy.\n",
        "- The significant difference in update frequencies and Q-values across the two methods highlights the trade-off between focusing on high-level strategies (options) versus detailed, action-level decisions in hierarchical reinforcement learning."
      ],
      "metadata": {
        "id": "t99TFsY_UVQ9"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lfhw_NFLURKL"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}