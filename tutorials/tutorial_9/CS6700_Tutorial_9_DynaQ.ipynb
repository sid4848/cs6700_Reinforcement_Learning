{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 9: DynaQ\n",
    "\n",
    "### Tasks to be done:\n",
    "\n",
    "1. Complete code for Planning step update. (search for \"TODO\" marker)\n",
    "2. Compare the performance (train and test returns) for the following values of planning iterations = **[0, 1, 2, 5, 10]**\n",
    "3. For each value of planning iteration, average the results on **100 runs** (due to the combined stochasticity in the env, epsilon-greedy and planning steps, we need you to average the results over a larger set of runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "COS_Qb1mjx4h",
    "outputId": "fd9bd270-af58-46c4-cdc6-d419a770b177"
   },
   "outputs": [],
   "source": [
    "!pip install gymnasium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_PsSaks36VVs"
   },
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import random\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "id": "aZFZd_nx6TBs",
    "outputId": "aac5e26e-dc5b-4fc8-d593-614b11f52813"
   },
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v1', is_slippery = True, render_mode = 'rgb_array')\n",
    "env.reset()\n",
    "\n",
    "# https://gymnasium.farama.org/environments/toy_text/frozen_lake\n",
    "\n",
    "# if pygame is not installed run: \"!pip install gymnasium[toy-text]\"\n",
    "\n",
    "plt.imshow(env.render())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "acTXxGVfkj6k"
   },
   "outputs": [],
   "source": [
    "class DynaQ:\n",
    "    def __init__(self, num_states, num_actions, gamma=0.99, alpha=0.01, epsilon=0.25):\n",
    "        self.num_states = num_states\n",
    "        self.num_actions = num_actions\n",
    "        self.gamma = gamma  # discount factor\n",
    "        self.alpha = alpha  # learning rate\n",
    "        self.epsilon = epsilon  # exploration rate\n",
    "        self.q_values = np.zeros((num_states, num_actions))  # Q-values\n",
    "        self.model = {}  # environment model, mapping state-action pairs to next state and reward\n",
    "        self.visited_states = []  # dictionary to track visited state-action pairs\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.choice(self.num_actions)\n",
    "        else:\n",
    "            return np.argmax(self.q_values[state])\n",
    "\n",
    "    def update_q_values(self, state, action, reward, next_state):\n",
    "        # Update Q-value using Q-learning\n",
    "        best_next_action = np.argmax(self.q_values[next_state])\n",
    "        td_target = reward + self.gamma * self.q_values[next_state][best_next_action]\n",
    "        td_error = td_target - self.q_values[state][action]\n",
    "        self.q_values[state][action] += self.alpha * td_error\n",
    "\n",
    "    def update_model(self, state, action, reward, next_state):\n",
    "        # Update model with observed transition\n",
    "        self.model[(state, action)] = (reward, next_state)\n",
    "\n",
    "    def planning(self, plan_iters):\n",
    "        # Perform planning using the learned model\n",
    "        for _ in range(plan_iters):\n",
    "            # TODO\n",
    "            # WRITE CODE HERE FOR TASK 1\n",
    "            # Update q-value by sampling state-action pairs\n",
    "            pass\n",
    "\n",
    "    def sample_state_action(self):\n",
    "        # Sample a state-action pair from the dictionary of visited state-action pairs\n",
    "        state_action = random.sample(self.visited_states, 1)\n",
    "        state, action = state_action[0]\n",
    "        return state, action\n",
    "\n",
    "    def learn(self, state, action, reward, next_state, plan_iters):\n",
    "        # Update Q-values, model, and perform planning\n",
    "        self.update_q_values(state, action, reward, next_state)\n",
    "        self.update_model(state, action, reward, next_state)\n",
    "\n",
    "        # Update the visited state-action value\n",
    "        self.visited_states.append((state, action))\n",
    "        self.planning(plan_iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "awdGJ-AJ2lFv"
   },
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, env, gamma = 0.99, alpha = 0.01, epsilon = 0.25):\n",
    "        self.env = env\n",
    "        self.agent = DynaQ(env.observation_space.n, env.action_space.n, gamma, alpha, epsilon)\n",
    "\n",
    "    def train(self, num_episodes = 1000, plan_iters = 10):\n",
    "        # training the agent\n",
    "        all_returns = []\n",
    "        for episode in range(num_episodes):\n",
    "            state, _ = self.env.reset()\n",
    "            done = False\n",
    "            episodic_return = 0\n",
    "            while not done:\n",
    "                action = self.agent.choose_action(state)\n",
    "                next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
    "                episodic_return += reward\n",
    "                self.agent.learn(state, action, reward, next_state, plan_iters)\n",
    "                state = next_state\n",
    "                done = terminated or truncated\n",
    "            all_returns.append(episodic_return)\n",
    "\n",
    "        return all_returns\n",
    "\n",
    "    def test(self, num_episodes=500):\n",
    "        # testing the agent\n",
    "        all_returns = []\n",
    "        for episode in range(num_episodes):\n",
    "            episodic_return = 0\n",
    "            state, _ = self.env.reset()\n",
    "            done = False\n",
    "            while not done:\n",
    "                action = np.argmax(self.agent.q_values[state]) # Act greedy wrt the q-values\n",
    "                next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
    "                episodic_return += reward\n",
    "                state = next_state\n",
    "                done = terminated or truncated\n",
    "            all_returns.append(episodic_return)\n",
    "        return all_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GAyXHDrT2mWk"
   },
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "env = gym.make('FrozenLake-v1', is_slippery = True)\n",
    "agent = Trainer(env, alpha=0.01, epsilon=0.25)\n",
    "train_returns = agent.train(num_episodes = 1000, plan_iters = 10)\n",
    "eval_returns = agent.test(num_episodes = 1000)\n",
    "print(sum(eval_returns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRITE CODE HERE FOR TASKS 2 & 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yNiEk8N83OZP"
   },
   "source": [
    "TODO:\n",
    "- Compare the performance (train and test returns) for the following values of planning iterations = [0, 1, 2, 5, 10]\n",
    "- For each value of planning iteration, average the results on 100 runs (due to the combined stochasticity in the env, epsilon-greedy and planning steps, we need you to average the results over a larger set of runs)\n",
    "\n",
    "---\n",
    "\n",
    "Sample Skeleton Code:\n",
    "\n",
    "for pi in plan_iter: \n",
    "\n",
    "&emsp; for 100 times:\n",
    "\n",
    "&emsp;&emsp; train(pi) \n",
    "\n",
    "&emsp;&emsp; test() \n",
    "\n",
    "&emsp; print(avg_performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
